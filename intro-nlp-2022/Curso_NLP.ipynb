{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Curso_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DlwDFEJjGecu",
        "iCxagi9OHCZA",
        "x63UgYW7HMKD",
        "dnJl7U6lHkJr",
        "3ahZ5cuqHhDy",
        "n6RsKdYbIbTO",
        "ZLffD3ttHdn4"
      ],
      "authorship_tag": "ABX9TyNF4Zlo9OBHIUwRPTubRKi+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chemaar/python-programming-course/blob/master/intro-nlp-2022/Curso_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî∞ Arrancando con el Procesamiento del Lenguaje Natural\n"
      ],
      "metadata": {
        "id": "EeXFnqYrx75u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplos divertidos\n",
        "\n",
        "* *DALL¬∑E 2 is a new AI system that can create realistic images and art from a description in natural language.* https://openai.com/dall-e-2/ \n",
        "  * https://huggingface.co/spaces/dalle-mini/dalle-mini\n",
        "\n",
        "* \"Darth vader riding a white horse\"\n",
        "\n",
        "![Generado con Dall-E mini](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/darth_vader_white_horse.png)\n",
        "\n",
        "* \"Darth vader harvesting strawberries\"\n",
        "\n",
        "![Generado con Dall-E mini](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/darth_vader_strawberries.png)\n",
        "\n",
        "\n",
        "* \"restaurant in the isengard tower\"\n",
        "\n",
        "![Generado con Dall-E mini](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/isengard.png)\n",
        "\n",
        "\n",
        "* \"mickey mouse doing burlesque\"\n",
        "\n",
        "![Generado con Dall-E mini](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/mickey.png)"
      ],
      "metadata": {
        "id": "vfsNjj-XZAxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ùì Algunas preguntas iniciales...\n",
        "\n",
        "* ¬øAplicaciones de NLP?\n",
        "* ¬øFases en NLP?\n",
        "* ¬øModelos de lenguaje: GPT-3, Bloom, BERT, etc.?\n",
        "* ¬øProgramaci√≥n?"
      ],
      "metadata": {
        "id": "qi3KPIriKR9v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìó Definiciones\n",
        "\n",
        "**Procesamiento del lenguaje natural (*Natural Language Processing*). Es un conjunto de t√©cnicas en el √°mbito de la inteligencia artificial y la computaci√≥n linguistica para habilitar la interacci√≥n entre personas y m√°quinas mediante la programaci√≥n que permitan a la m√°quina \"entender\" el universo del discurso.**\n",
        "\n",
        "Otros aspectos:\n",
        "\n",
        "* *Natural Language Understanding* \n",
        "* *Natural Language Generation*\n",
        "* *Name Entity Recognition* \n",
        "* *Text mining* \n",
        "* *Lenguaje formal*\n",
        "* ...\n"
      ],
      "metadata": {
        "id": "UqzCyJC3LESA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¢ Casos de uso y aplicaciones\n",
        "\n",
        "Bas√°ndose en que el lenguaje es el medio de comunicaci√≥n entre personas, se pueden encontrar muchos casos de uso y aplicaciones que se podr√°n adaptar a los diferentes dominios.\n",
        "\n",
        "*  An√°lisis de contenidos\n",
        "   * Opini√≥n\n",
        "   * Sentimiento\n",
        "   * Reconocimiento de entidades: lugares, personas, unidades de medida, moneda, c√≥digos, acr√≥nimos, etc.\n",
        "   * Anotaci√≥n autom√°tica\n",
        "   * B√∫squeda de patrones de texto\n",
        "   * ...\n",
        "*  Generaci√≥n de contenidos\n",
        "   * Redacci√≥n autom√°tica de texto\n",
        "   * Resumen de texto\n",
        "   * Simplificaci√≥n y personalizaci√≥n de textos\n",
        "   * Verbalizaci√≥n de im√°genes\n",
        "   * Recomendaci√≥n de textos (ej: etiquetas)\n",
        "   * ...\n",
        "*  Interacci√≥n\n",
        "   * Bots de productividad\n",
        "   * Chatbots\n",
        "   * ...\n",
        "*  B√∫squeda\n",
        "   * Textual (tipo Google)\n",
        "   * Sem√°ntica: NLP + Sem√°ntica\n",
        "*  Traducci√≥n de texto\n",
        "*  Otros:\n",
        "   * Reconocimiento y sintetizaci√≥n de voz\n",
        "\n"
      ],
      "metadata": {
        "id": "sAze_E96M4d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üë∑ Tareas concretas\n",
        "* Comparar dos palabras\n",
        "* B√∫squeda de palabras similares\n",
        "* Correcci√≥n de errores l√©xicos/sint√°cticos\n",
        "* ..."
      ],
      "metadata": {
        "id": "rVGExNNOaih0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T√©cnicas y enfoques para el procesamiento de lenguaje natural\n",
        "\n",
        "* Computaci√≥n linguistica: codificaci√≥n en un modelo de programaci√≥n de las reglas l√©xicas, gramaticales y sem√°nticas de un lenguaje natural. ¬øC√≥mo se analiza un texto en un idioma determinado?\n",
        "* Reglas: implementaci√≥n del proceso de lenguaje natural mediante reglas.\n",
        "* Aprendizaje autom√°tico: para algunos casos de uso, an√°lisis de sentimiento, un clasificador puede ser una opci√≥n muy v√°lidad.\n",
        "* Deep Learning (transformers): en tendencia actualmente, codificaci√≥n del texto y sus relaciones mediante estructuras computables (ej: vectores) basadas en probabilidades.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "üìî **_NOTA:_** \n",
        "\n",
        "En general y dependiendo del problema es muy posible que haya que combinar varias t√©cnicas teniendo siempre como base el procesamiento de lenguaje natural para hacer el lenguaje computable, entendible por una m√°quina, y despu√©s explotarlo de la manera m√°s adecuada a nuestras necesidades.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hgpA2EV6SsoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ùì ¬øQu√© casos de uso y/o aplicaciones podr√≠amos encontrar en: comercio electr√≥nico, salud o medios de comunicaci√≥n?\n",
        "  * Veamos que hacen en el New York times: https://rd.nytimes.com/research/natural-language-understanding "
      ],
      "metadata": {
        "id": "AQ_EYk2uOUtS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problemas conocidos en el procesamiento del lenguaje natural\n",
        "\n",
        "Los problemas principales vienen derivados del aspecto humano del lenguaje que de alguna forma habr√° que resolver para que una m√°quina los pueda entender apropiadamente, de esta forma se pueden destacar algunos como:\n",
        "\n",
        "* Ambiguedad del propio del lenguaje (universo del discurso y sem√°ntica): l√©xica, sint√°ctica y sem√°ntica.\n",
        "* Omisi√≥n de elementos clave del contexto (ej: \"it\")\n",
        "* Iron√≠a y sarcasmo\n",
        "* Procesamiento de emojis\n",
        "* Errores propios en el texto\n",
        "* Textos coloquiales (jerga)\n",
        "* Vocabulario t√©cnico (espec√≠fico)\n",
        "* Acr√≥nimos\n",
        "* Falta de suficientes textos para algunos contextos\n"
      ],
      "metadata": {
        "id": "ObKIO7KvUkP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚è© Proceso y actividades en el procesamiento del lenguaje natural\n"
      ],
      "metadata": {
        "id": "-ThBny0cOjFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Tokenizaci√≥n\n",
        "* Lematizaci√≥n (Stemming)\n",
        "* POS (Part of Speech Tagging)\n",
        "* NER (Name Entity Recognition)\n"
      ],
      "metadata": {
        "id": "qNImZ3QKWBXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tecnolog√≠a\n",
        "\n",
        "*   [Python Spacy](https://spacy.io/) \n",
        "*   [Python NLTK (Natural Language Toolkit)](https://www.nltk.org/): m√°s orientado a la investigaci√≥n. \n",
        "*   [Core NLP](https://stanfordnlp.github.io/CoreNLP/): m√°s orientado a la investigaci√≥n. \n",
        "*   [Gate](https://gate.ac.uk/)\n",
        "*   [Python Keras (Deep Learning) para NLP](https://keras.io/keras_nlp/).\n",
        "*   [Python Pytorch (Deep Learning) para NLP (Facebook)](https://pytorch.org/tutorials/beginner/deep_learning_nlp_tutorial.html). \n",
        "*   Apache Lucene, Solr, ElasticSearch (espec√≠fico para b√∫squeda). \n",
        "*   [Python Sci-kit text-analytics](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html): general para tareas de aprendizaje autom√°tico pero con muchas capacidades para NLP.\n",
        "*  Otras muchas tecnolog√≠as ya disponibles como productos configurables en Google u otros proveedores de software de an√°lisis de datos.\n",
        "\n",
        "## Modelos fundacionales\n",
        "*  [HugginFace](https://huggingface.co/models): repositorio de modelos. \n",
        "*  [GTP-3 de OpenAI](https://openai.com/blog/openai-api/). \n",
        "*  [BERT de Google](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)\n",
        "*  Glove, Gensim, etc.\n",
        "*  [MarIA de MareNostrum y BNE](https://github.com/PlanTL-GOB-ES/lm-spanish)\n",
        "\n",
        "üìî Lectura recomendada: \"On the Opportunities and Risks of\n",
        "Foundation Models\". [Ver PDF](https://arxiv.org/pdf/2108.07258.pdf)\n",
        "\n",
        "üì∞ Noticia interesante: [\"200 languages within a single AI model: A breakthrough in high-quality machine translation\"](https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "I8Vu6-qDWUAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚è¨ Procesamiento de lenguaje natural \"puro\" en Python"
      ],
      "metadata": {
        "id": "-s8MZr57cx45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supongamos que tenemos un texto como el siguiente (extra√≠do de la Wikipedia):\n",
        "\n",
        "*Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.*\n",
        "\n",
        "üë∑ Y se pretende realizar las siguientes tareas:\n",
        "\n",
        "* Extraer estad√≠sticas del texto: n¬∫ de palabras, n¬∫ de caracteres, palabra m√°s frecuente, etc.\n",
        "* Extraer los nombres propios\n",
        "* Extraer los verbos\n",
        "\n",
        "‚ùì ¬øC√≥mo se puede proceder?\n",
        "\n",
        "1. Tokenizar el texto\n",
        "2. ¬øEstrategia para las signos de puntuaci√≥n?\n",
        "3. ¬øSe pasa el texto a min√∫sculas/may√∫sculas?\n",
        "4. ¬øEstrategia con las \"stopwords\"?\n",
        "5. ¬øC√≥mo se resuelven los acr√≥nimos?"
      ],
      "metadata": {
        "id": "OSA7DJOcc1hA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducci√≥n a los procesos b√°sicos de NLP con Python NLTK"
      ],
      "metadata": {
        "id": "_yE9rcfdLG9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalizaci√≥n**\n",
        "\n",
        "Proceso por el cual se crea una versi√≥n \"limpia\" del texto. Por ejemplo, pasand a min√∫sculas, eliminando signos de puntuaci√≥n, etc. No obstante, hay que tener cuidado porque se puede perder sem√°ntica en el proceso, por ejemplo los acr√≥nimos."
      ],
      "metadata": {
        "id": "laV7omr3GPxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "text = \"Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.\"\n",
        "text = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
        "text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMoYK19xGW0Q",
        "outputId": "99bba677-59e9-4482-d8ab-b0370de7ba19"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jaca es la capital de la comarca de la jacetania y dista 72 km de huesca la capital provincial y 143 km de zaragoza est situada en el norte de la provincia en el valle del aragn nico gran valle paralelo al eje de la cadena pirenaica la prolongacin de este eje desde la cuenca de pamplona al oeste hasta la cuenca de tremp al este facilita las comunicaciones entre navarra y catalua a travs del norte de aragn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenizaci√≥n**\n",
        "\n",
        "Proceso por el cual se obtienen los tokens (palabras) de un texto. Existen diferentes t√©cnicas de tokenizaci√≥n que suelen estar basadas en la separaci√≥n por espacio en blanco."
      ],
      "metadata": {
        "id": "DlwDFEJjGecu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.\"\n",
        "tokens = input_text.split(\" \")\n",
        "print(\"N¬∫ de caracteres: \"+str(len(input_text)))\n",
        "print(\"N¬∫ de palabras: \"+str(len(tokens)))\n",
        "#Huesca, -->es una palabra?\n",
        "#Nombres propios-->son los que empiezan por may√∫scula\n",
        "nouns =[token for token in tokens if token[0].isupper()]\n",
        "print(\"N¬∫ de nombres propios: \"+str(len(nouns)))\n",
        "print(nouns)\n",
        "#y \"La Jacetania\"?\n",
        "n_numbers =[token for token in tokens if token.isnumeric()]\n",
        "print(\"N¬∫ de n√∫meros: \"+str(len(n_numbers)))\n",
        "print(n_numbers)\n",
        "#y los acr√≥nimos?\n",
        "#y los verbos?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoRyyT9Dd6TY",
        "outputId": "74469409-b6aa-4b95-a0bb-258260b57ed0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N¬∫ de caracteres: 427\n",
            "N¬∫ de palabras: 79\n",
            "N¬∫ de nombres propios: 15\n",
            "['Jaca', 'La', 'Jacetania', 'Huesca,', 'Zaragoza.', 'Est√°', 'Arag√≥n,', 'La', 'Cuenca', 'Pamplona,', 'Cuenca', 'Tremp,', 'Navarra', 'Catalu√±a', 'Arag√≥n.']\n",
            "N¬∫ de n√∫meros: 2\n",
            "['72', '143']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîú Vamos a introducir un poco de NLTK..."
      ],
      "metadata": {
        "id": "4QFrAhljkeVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "_HUgGWIsPazO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk import sent_tokenize\n",
        "from nltk import word_tokenize\n",
        "input_text = \"Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.\"\n",
        "sentences = sent_tokenize(input_text)\n",
        "print(\"Frases: \"+str(len(sentences)))\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "O2Wh1yf1GkGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "spanish_stopwords = stopwords.words('spanish')\n",
        "words = [token.replace(\".\",\"\") for token in tokens if token not in spanish_stopwords]\n",
        "print(\"N¬∫ de palabras: \"+str(len(words)))\n",
        "print(words)\n",
        "#Creando mi dataset de words\n",
        "real_words = []\n",
        "position = 0\n",
        "while position < len(words):\n",
        "  if position+1 < len(words)-1 and words[position][0].isupper() and words[position+1][0].isupper():\n",
        "    real_words.append(words[position]+\" \"+words[position+1])\n",
        "    position += 1\n",
        "  else:\n",
        "    real_words.append(words[position])\n",
        "  position += 1\n",
        "\n",
        "print(\"N¬∫ de palabras: \"+str(len(real_words)))\n",
        "print(real_words)  \n",
        "\n",
        "#Se deben incluir reglas muy imperativas.\n",
        "#-->y esto s√≥lo para particionar m√°s o menos bien, si los nombres son de m√°s de dos palabras?\n"
      ],
      "metadata": {
        "id": "bQY0--fVfLl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Resoluci√≥n de acr√≥nimos\n",
        "acronym_map = {\"km\": \"kilometro\"}\n",
        "acronyms = {token for token in tokens if token in acronym_map}\n",
        "print(acronyms)\n"
      ],
      "metadata": {
        "id": "8JtQUjd5fzO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Algunas estad√≠sticas\n",
        "unique_words = set(words)\n",
        "print(\"N¬∫ √∫nico de palabras: \"+str(len(unique_words)))\n",
        "#Frequencia de las palabras\n",
        "stats  = {word:words.count(word) for word in unique_words}\n",
        "print(stats)\n",
        "#Top 5\n",
        "top_5_most_frequent = sorted(stats.items(), key=lambda x: x[1], reverse=True)[:5]   \n",
        "print(top_5_most_frequent)"
      ],
      "metadata": {
        "id": "60roFjYtiXpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "nltk.download('punkt')\n",
        "tokenized_word=word_tokenize(input_text)\n",
        "print(tokenized_word)\n",
        "\n",
        "fdist = FreqDist(tokenized_word)\n",
        "print(fdist)\n",
        "print(fdist.most_common(5))\n"
      ],
      "metadata": {
        "id": "uhsHblpljpXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "fdist.plot(30,cumulative=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1pNBr-VakOVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "input_text = \"Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.\"\n",
        "wordcloud = WordCloud().generate(input_text)\n",
        "plt.figure(figsize = (10,10))\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eKQvkXhZG2cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Palabras similares?\n",
        "#Jaccard para conjuntos?\n",
        "def jaccard_similarity(s1, s2):\n",
        "    \"\"\"\n",
        "    Given two sets, it calculates the Jaccard similarity index. \n",
        "    \n",
        "    The Jaccard similarity index is calculated as follows:\n",
        "        \n",
        "        J(X,Y) = |X‚à©Y| / |X‚à™Y|\n",
        "\n",
        "    X = {0,1,2,5}\n",
        "    Y = {0,2,3,4}\n",
        "    \n",
        "    Solution: J(X,Y) = |X‚à©Y| / |X‚à™Y| = |{0,2}| / |{0,1,2,3,4,5}| = 2/6 = 0.33.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Float\n",
        "        The Jaccard similarity index.\n",
        "\n",
        "    \"\"\"\n",
        "    if s1 and s2:\n",
        "        intersection_cardinality = len(set.intersection(s1, s2))\n",
        "        union_cardinality = len(set.union(s1, s2))\n",
        "        return intersection_cardinality/float(union_cardinality)\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "word_1 = set(\"casa\")\n",
        "word_2 = set(\"cosa\")\n",
        "#Una similaridad simplemente basada en conjuntos, muy pobre, no considera ninguna sem√°ntica.\n",
        "print(\"Similaridad: \"+str(jaccard_similarity(word_1, word_2)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jPoAJrrpPUb",
        "outputId": "26f6e540-6324-4656-ab0d-a087493b12c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridad: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import SequenceMatcher\n",
        "def similar(a, b):\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "word_1 = \"casa\"\n",
        "word_2 = \"cosa\"\n",
        "print(\"Similaridad: \"+str(similar(word_1, word_2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwYsiARC3NZ0",
        "outputId": "b556c5a0-e647-42b3-8413-ebdf927bb694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridad: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jellyfish"
      ],
      "metadata": {
        "id": "6EhKhhR63nSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paquete `jellyfish`:\n",
        "\n",
        "* String comparison:\n",
        "\t* Levenshtein Distance\n",
        "\t* Damerau-Levenshtein Distance\n",
        "\t* Jaro Distance\n",
        "\t* Jaro-Winkler Distance\n",
        "\t* Match Rating Approach Comparison\n",
        "\t* Hamming Distance\n",
        "\n",
        "* Phonetic encoding:\n",
        "\t* American Soundex\n",
        "\t* Metaphone\n",
        "\t* NYSIIS (New York State Identification and Intelligence System)\n",
        "\t* Match Rating Codex"
      ],
      "metadata": {
        "id": "wEfwvHWu34R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#La distancia de Levenshtein, distancia de edici√≥n o distancia entre palabras es el \n",
        "#n√∫mero m√≠nimo de operaciones requeridas para transformar una cadena de caracteres en otra.\n",
        "\n",
        "import jellyfish\n",
        "word_1 = \"casa\"\n",
        "word_2 = \"cosa\"\n",
        "jellyfish.levenshtein_distance(word_1, word_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U1S1Nwp3daq",
        "outputId": "e474ac0d-1466-4280-9949-26cfd8bc5871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**\n",
        "\n",
        "Proceso por el cual se obtienen las ra√≠ces de las palabras."
      ],
      "metadata": {
        "id": "iCxagi9OHCZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "#Hay muchos, normalmente Porter o Snowball son los m√°s utilizados\n",
        "ps = PorterStemmer()\n",
        "words = [\"drink\", \"drunk\", \"drinking\", \"drinks\"]\n",
        "for w in words:\n",
        "  print (ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzXembHyHEJc",
        "outputId": "0333f6e8-90c4-4786-f7b3-9a8acc2cd2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drink\n",
            "drunk\n",
            "drink\n",
            "drink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lematizaci√≥n**\n",
        "\n",
        "Proceso por el cual se obtiene una ra√≠z com√∫n de las palabras pero teniendo en cuenta el contexto de las palabras. Requiere m√°s tiempo pero se m√°s preciso."
      ],
      "metadata": {
        "id": "x63UgYW7HMKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNJnhnewHOB8",
        "outputId": "9a04b478-3ad4-47c8-f37b-a7fa1a4749ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "ps = PorterStemmer()\n",
        "wl = WordNetLemmatizer()\n",
        "print(ps.stem(\"memories\"))\n",
        "print(wl.lemmatize(\"memories\"))\n",
        "print(ps.stem(\"wolves\"))\n",
        "print(wl.lemmatize(\"wolves\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtsIEZ1mHP0Z",
        "outputId": "bf70607e-25de-4bb0-cc19-1f02c8cd3479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "memori\n",
            "memory\n",
            "wolv\n",
            "wolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chunking**\n",
        "\n",
        "Proceso por el cual se extraen ciertas partes interesantes de un texto. Se establece un patr√≥n para extraer y se obtiene el texto que encaja. En su versi√≥n m√°s sencilla puede ser con expresiones regulares, aunque lo ideal es utilizar el POS Tagging."
      ],
      "metadata": {
        "id": "dnJl7U6lHkJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"The White House is waiting for a new President.\"\n",
        "#Normalizamos\n",
        "text = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
        "#Quitamos stopwords\n",
        "words = [word for word in word_tokenize(text) if word not in stop_words]\n",
        "#Tokenizamos con POS Tagging\n",
        "pos_text = word_tokenize(\" \".join(words))\n",
        "tagged_words = nltk.pos_tag(pos_text)\n",
        "print(tagged_words)\n",
        "#Pattern\n",
        "pattern = \"NP: {<JJ><NN>}\"\n",
        "parser = nltk.RegexpParser(pattern)\n",
        "output = parser.parse(tagged_words)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "VqB927OJHmyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chinking**\n",
        "\n",
        "Proceso por el cual se borrar ciertas partes interesantes de un texto. "
      ],
      "metadata": {
        "id": "3ahZ5cuqHhDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Quitar stopwords\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words(\"english\")\n",
        "print(stop_words)\n",
        "#Habitualmente este conjunto tambi√©n lo hay que parametrizar"
      ],
      "metadata": {
        "id": "JV6R3teAH8ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "cQDeMG1RIBkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.chunk.regexp import ChunkRule\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.chunk import RegexpChunkParser \n",
        "text = \"The White House is waiting for a new President.\"\n",
        "#Normalizamos\n",
        "text = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
        "#Quitamos stopwords\n",
        "words = [word for word in word_tokenize(text) if word not in stop_words]\n",
        "#Tokenizamos con POS Tagging\n",
        "pos_text = word_tokenize(\" \".join(words))\n",
        "tagged_words = nltk.pos_tag(pos_text)\n",
        "print(tagged_words)\n",
        "#Pattern\n",
        "ur = ChunkRule('<NN>', 'single noun') \n",
        "parser = nltk.RegexpChunkParser([ur])\n",
        "output = parser.parse(tagged_words)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "o4xLYMgbHi31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POS Tagging**\n",
        "\n",
        "Proceso por el cual se asignan las categor√≠as sint√°cticas a las diferentes palabras."
      ],
      "metadata": {
        "id": "n6RsKdYbIbTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')"
      ],
      "metadata": {
        "id": "eORteoxGIdMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Most senior Democrats and former Justice Department officials agree a top contender for the position is Sally Q. Yates, the former deputy attorney general whose tenure stretched from 2015 to the early, tumultuous days of the Trump administration. Other names under consideration include Sen. Doug Jones (D-Ala.), former homeland security secretary Jeh Johnson, former Massachusetts governor Deval Patrick, California Attorney General Xavier Becerra and former White House adviser Lisa Monaco. Behind the scenes, each Democratic contender has a constituency as well as detractors. But whomever Biden picks will have to be confirmed by a Senate that is currently controlled by Republicans, and take command of a department wracked by accusations of politicization.\"\n",
        "#Normalizamos\n",
        "text = \"\".join(v for v in text if v not in string.punctuation).lower()\n",
        "#Quitamos stopwords\n",
        "words = [word for word in word_tokenize(text) if word not in stop_words]\n",
        "#Tokenizamos con POS Taggin\n",
        "pos_text = word_tokenize(\" \".join(words))\n",
        "nltk.pos_tag(pos_text)"
      ],
      "metadata": {
        "id": "QnaD9NIrIelw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sem√°ntica con Wordnet**\n",
        "\n",
        "Proceso por el cual se consulta el significado de las palabras. En este caso utilizando una ontolog√≠a o, mejor dicho, una base de datos l√©xica como Wordnet."
      ],
      "metadata": {
        "id": "ZLffD3ttHdn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "U4z56SnRIK4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "for words in wordnet.synsets(\"Happy\"):\n",
        "  for lemma in words.lemmas():\n",
        "    print(lemma)\n",
        "  print()\n",
        "\n",
        "word = wordnet.synsets(\"Happy\")[0]\n",
        "print(word.name())\n",
        "print(word.definition())\n",
        "print(word.examples())\n",
        "print(word.hypernyms())\n",
        "print(word.hyponyms())\n",
        "\n",
        "\n",
        "syns = []\n",
        "ants = []\n",
        "for word in wordnet.synsets(\"New\"):\n",
        "  for lemma in word.lemmas():\n",
        "    syns.append(lemma.name())\n",
        "    if lemma.antonyms():\n",
        "      ants.append(lemma.antonyms()[0].name())\n",
        "print(syns)\n",
        "print(ants)"
      ],
      "metadata": {
        "id": "Zx3N8I_WHbDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö¶ Evaluaci√≥n inicial\n",
        "\n",
        "* Operaciones b√°sicas para gestionar el nivel l√©xico pueden llegar a complicarse enormemente dependiendo del contexto. Por ejemplo: acr√≥nimos.\n",
        "* Reconocimiento de entidades \"a mano\" require de fuente de conocimiento externa. Por ejemplo: un listado de entidades a reconocer, nombres, lugares, etc.\n",
        "* Las reglas gramaticales se deben codificar \"a mano\".\n",
        "* La sem√°ntica del lenguaje no queda recogida de ninguna forma.\n",
        "* No existe una forma de representar el texto y sus relaciones.\n",
        "* Dificultad en enlazar las tareas propias del procesamiento de lenguaje natural.\n",
        "* Muchos de los problemas intr√≠nsecos al procesamiento del lenguaje natural no se resuelven f√°cilmente.\n",
        "\n",
        "**Necesidad de bibliotecas y configuraci√≥n.**\n",
        "\n",
        "* NLP tradicional requiere comprensi√≥n del contexto.\n",
        "* Un proceso de configuraci√≥n (manual).\n",
        "* Ontolog√≠as para mejorar la sem√°ntica.\n",
        "* Predictivo e interpretable.\n",
        "* No muchos datos.\n",
        "* R√°pido."
      ],
      "metadata": {
        "id": "iDBrDdC94bwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚è´ Procesamiento de lenguaje natural con Python Spacy"
      ],
      "metadata": {
        "id": "qZSSngMJWM73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Industrial-Strength Natural Language Processing\n",
        "* Orientado a la generaci√≥n de soluciones de procesamiento de lenguaje natural.\n",
        "* No es un API, es una biblioteca de funcionalidades habituales.\n",
        "\n",
        "Algunas caracter√≠sticas:\n",
        "\n",
        "* ‚ùé Support for 66+ languages\n",
        "* ‚ùé 73 trained pipelines for 22 languages\n",
        "* ‚ùé Multi-task learning with pretrained transformers like BERT\n",
        "* ‚ùé Pretrained word vectors\n",
        "* ‚ùé State-of-the-art speed\n",
        "* ‚ùé Production-ready training system\n",
        "* ‚ùé Linguistically-motivated tokenization\n",
        "* ‚ùé Components for named entity recognition, part-of-speech tagging, dependency parsing, sentence segmentation, text classification, lemmatization, morphological analysis, entity linking and more\n",
        "* ‚ùé Easily extensible with custom components and attributes\n",
        "* ‚ùé Support for custom models in PyTorch, TensorFlow and other frameworks\n",
        "* ‚ùé Built in visualizers for syntax and NER\n",
        "* ‚ùé Easy model packaging, deployment and workflow management\n",
        "* ‚ùé Robust, rigorously evaluated accuracy"
      ],
      "metadata": {
        "id": "_22RgGG-kpbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Modelo de procesamiento basado en un [pipeline](https://spacy.io/usage/processing-pipelines).\n",
        "\n",
        "![Imagen oficial](https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg)\n",
        "\n",
        "\n",
        "* Ver documentaci√≥n: https://spacy.io/usage/processing-pipelines\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "73PkIPRW60xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "0KE4tYzu6vVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R946OyoGw-e_"
      },
      "outputs": [],
      "source": [
        "#pip install -U spacy\n",
        "#python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "\n",
        "#Load Spanish tokenizer, tagger, parser and NER\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "#Procesamiento de texto (tambi√©n por l√≠neas)\n",
        "input_text = \"Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.\"\n",
        "doc = nlp(input_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizar dependencias\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "Xhje4q3F8mwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#POS Tagging\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
      ],
      "metadata": {
        "id": "8wA4UacJARHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Morfolog√≠a\n",
        "for token in doc:\n",
        "  print(token.text+ \"  \"+str(token.morph))"
      ],
      "metadata": {
        "id": "8zx6htUo-oJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lematizaci√≥n\n",
        "for token in doc:\n",
        "  print(token.text+ \"  \"+token.lemma_)"
      ],
      "metadata": {
        "id": "q4gX6eVg_P1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sintaxis\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbos:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])"
      ],
      "metadata": {
        "id": "tNKUi7YU75u5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec2d0c6-57f4-4623-948a-fafda014c747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun phrases: ['Jaca', 'la comarca', 'La Jacetania', 'dista', 'km', 'Huesca', ', la capital provincial', 'km', 'Zaragoza', 'el norte', 'la provincia', ', en el valle', 'Arag√≥n', ', √∫nico gran valle', 'paralelo al eje', 'la cadena pirenaica', 'La prolongaci√≥n', 'este eje', ', desde la Cuenca de Pamplona, al oeste', ', hasta la Cuenca de Tremp', ', al este', 'las comunicaciones', 'Navarra', 'Catalu√±a', 'trav√©s del norte', 'Arag√≥n']\n",
            "Verbos: ['facilitar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Chunk de nombres\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
      ],
      "metadata": {
        "id": "UjgAARQY_maz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#√Årbol de parseado\n",
        "for token in doc:\n",
        "    print(token.text, token.dep_, token.head.text, token.head.pos_, [child for child in token.children])"
      ],
      "metadata": {
        "id": "xrL5baxU_0sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reconocimiento de entidades\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "luJ8hjof76z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizaci√≥n de texto etiquetado\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "Vb57ErUD8yvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizaci√≥n personalizada. Ver ejemplos: https://spacy.io/usage/linguistic-features#tokenization\n",
        "\n",
        "import spacy\n",
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"gimme that\")  # phrase to tokenize\n",
        "print([w.text for w in doc])  # ['gimme', 'that']\n",
        "\n",
        "# Add special case rule\n",
        "special_case = [{ORTH: \"gim\"}, {ORTH: \"me\"}]\n",
        "nlp.tokenizer.add_special_case(\"gimme\", special_case)\n",
        "\n",
        "# Check new tokenization\n",
        "print([w.text for w in nlp(\"gimme that\")])  # ['gim', 'me', 'that']"
      ],
      "metadata": {
        "id": "0EWDb1QxA3RF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6214b07-35f1-4ce0-b7a5-b847ea938312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gimme', 'that']\n",
            "['gim', 'me', 'that']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Representaci√≥n como un vector\n",
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "# Procesando texto\n",
        "doc = nlp(\"Dos coches est√°n aparcados en la universidad\")\n",
        "# Ver el vector\n",
        "coches_vector = doc[1].vector\n",
        "print(coches_vector)"
      ],
      "metadata": {
        "id": "f5p-Kdn9DFQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Similaridad de palabras\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "doc = nlp(\"coche y veh√≠culo\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)\n",
        "\n",
        "\n",
        "doc = nlp(\"coche y moto\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)\n",
        "\n",
        "\n",
        "doc = nlp(\"coche y √°rbol\")\n",
        "token1, token2 = doc[0], doc[2]\n",
        "\n",
        "similarity = token1.similarity(token2)\n",
        "print(similarity)"
      ],
      "metadata": {
        "id": "NZiBivvKDzxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Similaridad a nivel de frase\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc1 = nlp(\"Hoy hace un d√≠a caluroso\")\n",
        "doc2 = nlp(\"Es un d√≠a soleado\")\n",
        "\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)\n",
        "\n",
        "doc1 = nlp(\"Hoy hace un d√≠a caluroso\")\n",
        "doc2 = nlp(\"Es un d√≠a lluvioso\")\n",
        "\n",
        "similarity = doc1.similarity(doc2)\n",
        "print(similarity)"
      ],
      "metadata": {
        "id": "TG8MRxiyDcJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶æ Tendencias en procesamiento de lenguaje natural\n"
      ],
      "metadata": {
        "id": "Uy-5WvP-In6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Notas generales sobre Deep Learning\n",
        "\n",
        "* ¬øC√≥mo se puede aplicar DL para NLP?\n",
        "* ¬øC√≥mo se representan las palabras?\n",
        "\n",
        "Referencias:\n",
        "\n",
        "* http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
        "* http://jalammar.github.io/mit-analytics-lab-talk/"
      ],
      "metadata": {
        "id": "YUOZyqtcIszP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame(src='//www.slideshare.net/slideshow/embed_code/key/K23kAMkMyKHDUr', width=595, height=485)"
      ],
      "metadata": {
        "id": "7z2CZ3sBYJ3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Representaci√≥n de informaci√≥n textual\n",
        "\n",
        "* One-hot encoding: dado un vector de vocabulario donde cada palabra est√° en un posici√≥n, un texto se corresponde con un vector de 0's y 1's.\n",
        "  * Problema: no guarda el contexto entre las palabras\n",
        "\n",
        "* Word Embeddings: se representa la probabilidad de aparici√≥n de las palabras de forma conjunta con una ventana de tama√±o n.\n",
        "\n",
        "![Curso de Stanford NLP](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/word2vec.png)\n",
        "\n",
        "Fuente: [Curso de Stanford NLP](http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture01-wordvecs1.pd)\n",
        "\n",
        "\n",
        "Enlaces interesantes:\n",
        "\n",
        "   * Teor√≠a: http://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture01-wordvecs1.pdf\n",
        "   * [\"Efficient Estimation of Word Representations in Vector Space\"](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "   * [Glove](https://nlp.stanford.edu/projects/glove/)\n",
        "   * [Tutorial de Gooogle](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb)"
      ],
      "metadata": {
        "id": "UH8OlkpgEj1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Embeddings**\n"
      ],
      "metadata": {
        "id": "aabDk9_RIxYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ver en Projector las palabras similares a \"city\": https://projector.tensorflow.org/\n",
        "\n",
        "![Visualizando city](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/projector.png)"
      ],
      "metadata": {
        "id": "59Sd64E6Gg9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelos para la representaci√≥n de informaci√≥n en vectores\n",
        "\n",
        "* Skip-gram: dada una palabra, ¬øcu√°l es el conjunto de palabras en una ventana m√°s probable?\n",
        "* C-BOW: dado un contexto (conjunto de palabras), ¬øcu√°l es la palabra m√°s probable?\n",
        "\n",
        "![Curso de Stanford NLP](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/skip-gram.png)\n",
        "\n",
        "Fuente: [Exploiting Similarities among Languages for Machine Translation](https://arxiv.org/pdf/1309.4168v1.pdf)"
      ],
      "metadata": {
        "id": "ptP-Zt7BHWAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word embeddings: ejemplo gr√°fico**\n",
        "\n",
        "* Supongamos que queremos generar titulares deportivos...y tenemos\n",
        "  * Una muestra de titulares de peri√≥dicos\n",
        "* Necesitamos...\n",
        "  * Aprender c√≥mo escriben los periodistas y generar titulares"
      ],
      "metadata": {
        "id": "l3g5f93x5uhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El proceso ser√≠a:\n",
        "\n",
        "1.   Texto de ejemplo (se puede hacer preprocesamiento)\n",
        "\n",
        "![Texto de ejemplo](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/embed_text.png)\n",
        "\n",
        "2.  Generaci√≥n de pares con tama√±o de ventana 2.\n",
        "\n",
        "3.  Entrenamiento (generaci√≥n de embeddings).\n",
        "\n",
        "4.  Predicci√≥n de texto.\n",
        "\n",
        "\n",
        "**Esto funcionar√≠a s√≥lo para este contexto del partido.**\n",
        "\n",
        "Un partido de una competici√≥n NO es generaci√≥n de texto perse, sino m√°s bien eventos. As√≠ tambi√©n lo ve√≠an en Stanford:\n",
        "\n",
        "![Stanford StoryTelling](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/story_telling_1.png)\n",
        "\n",
        "![Stanford StoryTelling](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/story_telling_2.png)\n",
        "\n",
        "**Por ello, cambiamos el proceso.**\n",
        "\n",
        "1.   **Texto de ejemplo (se puede hacer preprocesamiento)**\n",
        "\n",
        "![Texto de ejemplo](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/embed_text.png)\n",
        "\n",
        "2.  **Generaci√≥n de pares con tama√±o de ventana 2.**\n",
        "\n",
        "![Texto de ejemplo](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/embed_pattern.png)\n",
        "\n",
        "3.  **Entrenamiento (generaci√≥n de embeddings).**\n",
        "\n",
        "![Texto de ejemplo](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/embed_result.png)\n",
        "\n",
        "4.  **Predicci√≥n de texto.**\n",
        "\n",
        "5.  **Enriquecimiento de texto con un modelo de lenguaje existente (ej: BERT).**\n",
        "\n",
        "\n",
        "**En este caso se definieron varios contextos y se aplic√≥ el mismo proceso...**\n",
        "\n",
        "* Objetivos del equipo y alineaci√≥n con clasificaci√≥n actual y resultado.\n",
        "* El tiempo (siempre relevante)\n",
        "* Los eventos del partido\n",
        "* ..."
      ],
      "metadata": {
        "id": "NX-4ZcTx6ESm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame(src='https://www.youtube.com/embed/xnOxJL6-D-E', width=560, height=315)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "eddyPfwk-A1C",
        "outputId": "28b0741e-8671-4a2f-c394-c7a106cd0fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f2c60011a90>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"560\"\n",
              "            height=\"315\"\n",
              "            src=\"https://www.youtube.com/embed/xnOxJL6-D-E\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word embeddings paso a paso**"
      ],
      "metadata": {
        "id": "uyFo_TZLsYj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fuente original: https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8\n",
        "\n",
        "\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Deep learning: \n",
        "from keras.models import Input, Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "#Utility\n",
        "\n",
        "def create_unique_word_dict(text:list) -> dict:\n",
        "    \"\"\"\n",
        "    A method that creates a dictionary where the keys are unique words\n",
        "    and key values are indices\n",
        "    \"\"\"\n",
        "    # Getting all the unique words from our text and sorting them alphabetically\n",
        "    words = list(set(text))\n",
        "    words.sort()\n",
        "\n",
        "    # Creating the dictionary for the unique words\n",
        "    unique_word_dict = {}\n",
        "    for i, word in enumerate(words):\n",
        "        unique_word_dict.update({\n",
        "            word: i\n",
        "        })\n",
        "\n",
        "    return unique_word_dict "
      ],
      "metadata": {
        "id": "vMc7vqWbtMHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "texts = [\"Un coche es un veh√≠culo\", \"Una moto es un veh√≠culo\", \"Un coche es r√°pido\", \"Una moto es menos r√°pida que un coche\"]\n",
        "window = 2\n",
        "\n",
        "word_lists = []\n",
        "all_text = []\n",
        "\n",
        "for text in texts:\n",
        "    text = [token.lower() for token in text.split()]\n",
        "    all_text += text \n",
        "    # Creating a context dictionary\n",
        "    for i, word in enumerate(text):\n",
        "        for w in range(window):\n",
        "            # Getting the context that is ahead by *window* words\n",
        "            if i + 1 + w < len(text): \n",
        "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
        "            # Getting the context that is behind by *window* words    \n",
        "            if i - w - 1 >= 0:\n",
        "                word_lists.append([word] + [text[(i - w - 1)]])\n",
        "\n",
        "unique_word_dict = create_unique_word_dict(all_text)\n",
        "\n",
        "# Defining the number of features (unique words)\n",
        "n_words = len(unique_word_dict)\n",
        "# Getting all the unique words \n",
        "words = list(unique_word_dict.keys())\n",
        "\n",
        "# Creating the X and Y matrices using one hot encoding\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i, word_list in tqdm(enumerate(word_lists)):\n",
        "    # Getting the indices\n",
        "    main_word_index = unique_word_dict.get(word_list[0])\n",
        "    context_word_index = unique_word_dict.get(word_list[1])\n",
        "\n",
        "    # Creating the placeholders   \n",
        "    X_row = np.zeros(n_words)\n",
        "    Y_row = np.zeros(n_words)\n",
        "\n",
        "    # One hot encoding the main word\n",
        "    X_row[main_word_index] = 1\n",
        "\n",
        "    # One hot encoding the Y matrix words \n",
        "    Y_row[context_word_index] = 1\n",
        "\n",
        "    # Appending to the main matrices\n",
        "    X.append(X_row)\n",
        "    Y.append(Y_row)\n",
        "\n",
        "X = np.asarray(X)\n",
        "Y = np.asarray(Y)\n",
        "\n",
        "# Defining the size of the embedding\n",
        "embed_size = 2 #This is the size of the window\n",
        "\n",
        "# Building the network\n",
        "inp = Input(shape=(X.shape[1],))\n",
        "x = Dense(units=embed_size, activation='linear')(inp)\n",
        "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
        "model = Model(inputs=inp, outputs=x)\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
        "\n",
        "# Optimizing the network weights\n",
        "model.fit(x=X, y=Y, batch_size=256, epochs=1000, verbose=False)\n",
        "\n",
        "# Obtaining the weights from the neural network. \n",
        "# These are the so called word embeddings\n",
        "\n",
        "# The input layer \n",
        "weights = model.get_weights()[0]\n",
        "\n",
        "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
        "# the value is the numeric vector\n",
        "embedding_dict = {}\n",
        "for word in words: \n",
        "    embedding_dict.update({\n",
        "        word: weights[unique_word_dict.get(word)]\n",
        "        })\n",
        "\n",
        "# Ploting the embeddings\n",
        "plt.figure(figsize=(10, 10))\n",
        "for word in list(unique_word_dict.keys()):\n",
        "    coord = embedding_dict.get(word)\n",
        "    plt.scatter(coord[0], coord[1])\n",
        "    plt.annotate(word, (coord[0], coord[1]))   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "kiKmrVSVvrfh",
        "outputId": "6cbc1ae6-6808-496a-c2ac-57a1a543440a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "64it [00:00, 129741.64it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Un ejemplo con Gensim..."
      ],
      "metadata": {
        "id": "F3wFVnNZyX_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Download embeddings (66MB, glove, trained on wikipedia)\n",
        "model = api.load(\"glove-wiki-gigaword-50\")"
      ],
      "metadata": {
        "id": "ZGeNHNadI0Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model['king']"
      ],
      "metadata": {
        "id": "vL2xfNVtI3sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embeddings(vectors, labels=None):\n",
        "    n_vectors = len(vectors)\n",
        "    fig = plt.figure(figsize=(12, n_vectors))\n",
        "    # ax = fig.add_axes([0.1, 0.1, 0.8, 0.8])\n",
        "    # ax = fig.add_axes([1, 1, 1, 1])\n",
        "    ax = plt.gca()\n",
        "    \n",
        "    sns.heatmap(vectors, cmap='RdBu', vmax=2, vmin=-2, ax=ax)\n",
        "    \n",
        "    if labels:\n",
        "        ax.set_yticklabels(labels,rotation=0)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=30)\n",
        "        \n",
        "    plt.tick_params(axis='x',          # changes apply to the x-axis\n",
        "        which='both',      # both major and minor ticks are affected\n",
        "        bottom=False,      # ticks along the bottom edge are off\n",
        "        top=False,         # ticks along the top edge are off\n",
        "        labelbottom=False) # labels along the bottom edge are off\n",
        "    \n",
        "    # From https://github.com/mwaskom/seaborn/issues/1773\n",
        "    # fix for mpl bug that cuts off top/bottom of seaborn viz\n",
        "    b, t = plt.ylim() # discover the values for bottom and top\n",
        "    b += 0.5 # Add 0.5 to the bottom\n",
        "    t -= 0.5 # Subtract 0.5 from the top\n",
        "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
        "    plt.show() # ta-da!\n",
        "\n",
        "plot_embeddings([model['king'], model['man'], model['woman'], model['girl'], model['boy']],\n",
        "              ['king', 'man', 'woman', 'girl', 'boy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "GjSQj0rmI6XJ",
        "outputId": "f79045b0-9a87-42cc-b09a-a76bcea36439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://ars.els-cdn.com/content/image/1-s2.0-S2666651021000231-gr9.jpg"
      ],
      "metadata": {
        "id": "F94mkXrPLVjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Transformers** y modelos de lenguaje\n",
        "\n",
        "* Visi√≥n:\n",
        "\n",
        "![Transformers](https://ars.els-cdn.com/content/image/1-s2.0-S2666651021000231-gr9.jpg)\n",
        "\n",
        "Fuente: [Pre-trained models: Past, present and future](https://www.sciencedirect.com/science/article/pii/S2666651021000231)\n",
        "\n",
        "* Tecnolog√≠a:\n",
        "\n",
        "![Tecnolog√≠a Transformers](https://ars.els-cdn.com/content/image/1-s2.0-S2666651021000231-gr4.jpg)\n",
        "\n",
        "* Evoluci√≥n: billones (miles de millones de par√°metros). GPT-3, LaMDA, PALM (540b), BLOOM (175b), MARIA, etc.\n",
        "\n",
        "![Evoluci√≥n](https://miro.medium.com/max/1400/0*6oaNF3UEOBHX_l-s)\n",
        "\n",
        "* T√©cnica que representa el estado del arte para procesamiento de lenguaje natural (y, en realidad, para muchos otros problemas).\n",
        "\n",
        "\n",
        "![Paper original](https://github.com/chemaar/python-programming-course/raw/master/intro-nlp-2022/imgs/transformer.png)\n",
        "\n",
        "Fuente: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "*The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.*\n",
        "\n",
        "\n",
        "Enlaces de inter√©s:\n",
        "\n",
        "* [Transformers en el blog de Google AI](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
        "* [Visualizando BERT](https://colab.research.google.com/drive/1hXIQ77A4TYS4y3UthWF-Ci7V7vVUoxmQ?usp=sharing)\n",
        "* [Google Palm](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)\n",
        "* [Bloom](https://huggingface.co/docs/transformers/model_doc/bloom) \n",
        "* [Lamda](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html)\n",
        "* [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)"
      ],
      "metadata": {
        "id": "tj8_usZMHA0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Explorando modelos fundacionales: BERT, GPT, etc. con Huggingface**\n",
        "\n",
        "* [Bidirectional Encoder Representations from Transformers](https://arxiv.org/abs/1810.04805) es un modelo de lenguaje creado por Google.\n",
        "\n",
        "\n",
        "* https://huggingface.co/PlanTL-GOB-ES/roberta-base-bne "
      ],
      "metadata": {
        "id": "Jy4u0z-bM9qU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq transformers"
      ],
      "metadata": {
        "id": "-GqihOiLMMMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24823ddb-b60e-43cf-b587-a2aa57cb229b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.4 MB 5.1 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 61.0 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 101 kB 11.2 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 28.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n"
      ],
      "metadata": {
        "id": "zWs9EGV3L_NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jaca is a city of northeastern Spain in the province of Huesca, located near the Pyrenees and the border with France. Jaca is an ancient fort on the Arag√≥n River, situated at the crossing of two great early medieval routes, one from Pau to Zaragoza. Jaca was the city out of which the County and Kingdom of Aragon developed. It was the capital of Aragon until 1097 and also the capital of Jacetania.\""
      ],
      "metadata": {
        "id": "ktg4eAPYL8OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "outputs = classifier(text)\n",
        "pd.DataFrame(outputs)"
      ],
      "metadata": {
        "id": "aP8-_qEXMay9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "0c1a68db-fb9e-4acc-8da9-e6134ce3f659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      label     score\n",
              "0  POSITIVE  0.994097"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a4ae533-8207-4097-9cff-693087c32b06\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>POSITIVE</td>\n",
              "      <td>0.994097</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a4ae533-8207-4097-9cff-693087c32b06')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a4ae533-8207-4097-9cff-693087c32b06 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a4ae533-8207-4097-9cff-693087c32b06');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "outputs = ner_tagger(text)\n",
        "pd.DataFrame(outputs)"
      ],
      "metadata": {
        "id": "ULEhiyp1Mggd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "question = \"Where Jaca is located?\"\n",
        "outputs = reader(question=question, context=text)\n",
        "pd.DataFrame([outputs])"
      ],
      "metadata": {
        "id": "dG3Th-TfMo1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "outputs = summarizer(text, max_length=60, clean_up_tokenization_spaces=True)\n",
        "print(outputs[0]['summary_text'])"
      ],
      "metadata": {
        "id": "oXECQudeMzgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2text = pipeline(\"text2text-generation\")\n",
        "text2text(\"question: Which is capital city of Spain?\")"
      ],
      "metadata": {
        "id": "uLAcFfEJTSrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2text(\"question: Which is capital city of Germany?\")"
      ],
      "metadata": {
        "id": "ddm5oGB1Tq6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2text(\"translate English to German: Jaca is a good place for summer\")"
      ],
      "metadata": {
        "id": "GRCHvEFlT3SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "text= generator(\"A football player must run and\", max_length=50, do_sample=False)[0]\n",
        "print(text['generated_text'])"
      ],
      "metadata": {
        "id": "0uBHFveWUgMJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3f1058-341a-4365-d6b6-fbc3e0aba0d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A football player must run and pass the ball.\n",
            "\n",
            "The ball must be thrown in the air.\n",
            "\n",
            "The ball must be thrown in the air.\n",
            "\n",
            "The ball must be thrown in the air.\n",
            "\n",
            "The ball must be thrown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text= generator(\"The summer in Spain is warm and\", max_length=50, do_sample=False)[0]\n",
        "print(text['generated_text'])"
      ],
      "metadata": {
        "id": "fwFmFxTlU0a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Jaca es la capital de la comarca de La Jacetania y dista 72 km de Huesca, la capital provincial, y 143 km de Zaragoza. Est√° situada en el norte de la provincia, en el valle del Arag√≥n, √∫nico gran valle paralelo al eje de la cadena pirenaica. La prolongaci√≥n de este eje, desde la Cuenca de Pamplona, al oeste, hasta la Cuenca de Tremp, al este, facilita las comunicaciones entre Navarra y Catalu√±a a trav√©s del norte de Arag√≥n.\"\n",
        "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\", model=\"PlanTL-GOB-ES/roberta-base-bne\")\n",
        "outputs = ner_tagger(input_text)\n",
        "pd.DataFrame(outputs)"
      ],
      "metadata": {
        "id": "s6BLKmkjWI8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚õ≥ Actividad\n",
        "\n",
        "* Tipo: grupo (2-3 personas)\n",
        "* Objetivo: dise√±ar un sistema de NLP para alg√∫n problema relacionado con las humanidades (preferentemente)\n",
        "* Tiempo: 15 minutos + 3 minutos presentaci√≥n\n",
        "* Entregable: 1-2 diapositvas/diagrama o similar indicando los siguientes aspectos:\n",
        "  * Historia de usuario\n",
        "    * `Como <actor> quiero <capacidad> para as√≠ <conseguir objetivo>`\n",
        "  * Acr√≥nimo\n",
        "  * Tipolog√≠a del problema: an√°lisis de contenido/generaci√≥n/etc.\n",
        "  * √Åmbito\n",
        "  * Necesidades de NLP\n",
        "  * Conjuntos de datos iniciales\n",
        "  * Reutilizaci√≥n de datos\n",
        "  * ¬øDemo?\n",
        "  "
      ],
      "metadata": {
        "id": "do2qVp12NQ29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplos de problemas\n",
        "\n",
        "*  Generaci√≥n de titulares de noticias para cualquier dominio.\n",
        "*  Generaci√≥n de noticias (storytelling)\n",
        "*  Topic modelling para Twitter\n",
        "*  An√°lisis de sentimiento\n",
        "*  An√°lisis de opini√≥n de encuestas\n",
        "*  Sistema de preguntas/respuestas para periodistas (JARVIS del periodismo)\n",
        "*  Personalizaci√≥n y explicaci√≥n de contenidos para un dominio determinado\n",
        "*  Otras apps: https://plantl.mineco.gob.es/tecnologias-lenguaje/actividades/desarrollos/Paginas/desarrollos-SW.aspx \n",
        "*  Ejemplos en el √°mbito de Big Data: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.1500-3r2.pdf \n",
        "*  ..."
      ],
      "metadata": {
        "id": "sNuOkdHtNh0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## An√°lisis de Twitter stream\n",
        "\n",
        "https://www.kaggle.com/code/thebrownviking20/topic-modelling-with-spacy-and-scikit-learn/notebook"
      ],
      "metadata": {
        "id": "iRscR8ezJt0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tweepy"
      ],
      "metadata": {
        "id": "8_WpTqyNXRkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "CONSUMER_KEY=\"\"\n",
        "CONSUMER_SECRET=\"\"\n",
        "ACCESS_TOKEN=\"\"\n",
        "ACCESS_TOKEN_SECRET=\"\"\n",
        "\n",
        "auth = tweepy.OAuth1UserHandler(\n",
        "   CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
        ")\n",
        "\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "public_tweets = api.home_timeline()\n",
        "for tweet in public_tweets:\n",
        "    print(tweet.text)"
      ],
      "metadata": {
        "id": "uIm1QqrlsFbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bibliograf√≠a de inter√©s y referencias"
      ],
      "metadata": {
        "id": "GTUz1T2Nw_Gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NLP with Deep Learning course (video lectures and exercises). URL: http://web.stanford.edu/class/cs224n/\n",
        "* Transformers united course (video lectures and exervices). URL: https://web.stanford.edu/class/cs25/\n",
        "* A Path Towards Autonomous Machine Intelligence. Yann LeCun, Junio 2022. URL: https://openreview.net/forum?id=BZ5a1r-kVsf \n",
        "* \"Yann LeCun on a vision to make AI systems learn and reason like animals and humans\". Febrero 2022. URL: https://ai.facebook.com/blog/yann-lecun-advances-in-ai-research/\n",
        "* https://www.kaggle.com/code/amananandrai/hugging-face-pipeline-examples/notebook\n",
        "* Natural Language Processing with Transformers: Building Language Applications with Hugging Face}\". 2022. Repositorio Github: https://github.com/nlp-with-transformers/notebooks\n",
        "* Transformers for Natural Language Processing - Second Edition. 2022. https://www.packtpub.com/product/transformers-for-natural-language-processing-second-edition/9781803247335\n",
        "* https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8"
      ],
      "metadata": {
        "id": "CUFr1yYvxG3b"
      }
    }
  ]
}